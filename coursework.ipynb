{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGlp4sb-8i3i"
      },
      "source": [
        "# Coursework: Self-supervised learning\n",
        "\n",
        "In this coursework, you will explore the popular self-supervised contrastive learning approach [SimCLR]((https://arxiv.org/abs/2002.05709)).\n",
        "\n",
        "You will be asked to implement some of the key components of SimCLR, including a suitable data augmentation strategy (for generating positive pairs), the SimCLR loss function, and the SimCLR training step. Additionally, you will be using transfer learning strategies for evaluating the performance of different pre-trained models for a downstream classification task.\n",
        "\n",
        "The coursework is divided into three-parts:\n",
        "- **Part A:** Implementation of a suitable dataset for contrastive model training;\n",
        "- **Part B:** Implementation of the SimCLR loss and training step;\n",
        "- **Part C:** Implementation of transfer learning strategies (linear probing and finetuning) for model evaluation.\n",
        "\n",
        "**Important:** Read the text descriptions carefully and look out for hints and comments indicating a specific 'TASK'. Make sure to add sufficient documentation to your code.\n",
        "\n",
        "**Submission:** You are asked to submit two versions of your notebook:\n",
        "1. You should submit the raw notebook in `.ipynb` format with *all outputs cleared*. Please name your file `coursework.ipynb`.\n",
        "2. Additionally, you will be asked to submit an exported version of your notebook in `.pdf` format, with *all outputs included*. We will primarily use this version for marking, but we will use the raw notebook to check for correct implementations. Please name this file `coursework_export.pdf`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxpFCs1SSOGF"
      },
      "source": [
        "## Your details\n",
        "\n",
        "Please add your details below. You can work in groups up to two.\n",
        "\n",
        "Authors: **Kangle Yuan** & **Jiqiu Hu**\n",
        "\n",
        "DoC alias: **ky523** & **jh523**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4nr-pFhSOGF"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhdSe7hJ8jqO"
      },
      "outputs": [],
      "source": [
        "# On Google Colab uncomment the following line to install PyTorch Lightning and\n",
        "#the MedMNIST dataset\n",
        "! pip install lightning medmnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJAj8P5r8i3m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from pytorch_lightning import LightningModule, LightningDataModule, Trainer,\n",
        "seed_everything\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
        "from torchmetrics.functional import auroc\n",
        "from PIL import Image\n",
        "from medmnist.info import INFO\n",
        "from medmnist.dataset import MedMNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL7v3hY78i3n"
      },
      "source": [
        "## **Part A:** Implement a dataset suitable for contrastive learning.\n",
        "\n",
        "We will be using the [MedMNIST Pneumonia](https://medmnist.com/) dataset, which is a medical imaging inspired dataset but with the characteristics of MNIST. This allows efficient experimentation due to the small image size. The dataset contains real chest X-ray images but downsampled to 28 x 28 pixels, with binary labels indicating the presence of [Pneumonia](https://www.nhs.uk/conditions/pneumonia/) (which is an inflammation of the lungs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2EAXIOzSOGJ"
      },
      "source": [
        "### **Task A-1:** Complete the dataset implementation.\n",
        "\n",
        "You are asked to implement a dataset class `SimCLRPneumoniaMNISTDataset` suitable for training a self-supervised model with a contrastive objective. For each sample, your dataset class should return two 'views' of the corresponding image, forming the positive pairs for contrastive learning. It is up to you to design suitable augmentation pipeline for generating these views. Please provide a short description in plain language of what your data augmentation pipeline is meant to do.\n",
        "\n",
        "To get you started, we have provided the skeleton of the dataset class in the cell below. Once you have implemented your dataset class, you are asked to run the provided visualisation code to visualise one batch of your training dataloader.\n",
        "\n",
        "*Note:* You can use the same data augmentation pipeline for training, validation, and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WpnXKjrm0bL"
      },
      "outputs": [],
      "source": [
        "class SimCLRPneumoniaMNISTDataset(MedMNIST):\n",
        "    def __init__(self, split = 'train'):\n",
        "        ''' Dataset class for PneumoniaMNIST.\n",
        "        The provided init function will automatically download the necessary\n",
        "        files at the first class initialistion.\n",
        "\n",
        "        :param split: 'train', 'val' or 'test', select subset\n",
        "\n",
        "        '''\n",
        "        self.flag = \"pneumoniamnist\"\n",
        "        self.size = 28\n",
        "        self.size_flag = \"\"\n",
        "        self.root = './data/coursework/'\n",
        "        self.info = INFO[self.flag]\n",
        "        self.download()\n",
        "\n",
        "        npz_file = np.load(os.path.join(self.root, \"pneumoniamnist.npz\"))\n",
        "\n",
        "        self.split = split\n",
        "\n",
        "        # Load all the images\n",
        "        assert self.split in ['train','val','test']\n",
        "\n",
        "        self.imgs = npz_file[f'{self.split}_images']\n",
        "        self.labels = npz_file[f'{self.split}_labels']\n",
        "\n",
        "        # TASK: Define here your data augmentation pipeline\n",
        "        # Add a short description in plain language.\n",
        "\n",
        "        # Random Rotation: rotate the image by a small angle, to\n",
        "        # mimic how patient lie unperfectly in real life\n",
        "\n",
        "        # Random Horizontal Flip: flip the image horizontally with a 50% chance,\n",
        "        # to mimic the different side of viewing the xray.\n",
        "\n",
        "        # Random Resized Crop: Crop a part of the image and resize it back to\n",
        "        # 28x28 pixels. This help the model to focus on different parts\n",
        "        # of the lung.\n",
        "\n",
        "        # Color Jitter: Adjust the brightness, saturation, hue and contrast of\n",
        "        #the image,to mimic the diffrence after processing of x-ray\n",
        "\n",
        "        # Gaussian Noise: add a small amount of Gaussian noise to mimic\n",
        "        # the sensor noise in X-ray machines.\n",
        "\n",
        "        self.augmentation_pipeline = transforms.Compose([\n",
        "            # convert to PIL image\n",
        "            transforms.ToPILImage(),\n",
        "            # randomly rotate image by degree from -3 to 3\n",
        "            transforms.RandomRotation(3),\n",
        "            # flip image horizontally with 50% probability\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            # crop random part of the image and then resize back\n",
        "            transforms.RandomResizedCrop(self.size, scale=(0.77, 0.92)),\n",
        "            #randomly changes brightness and contrast of the image within range\n",
        "\n",
        "\n",
        "            transforms.RandomApply([transforms.ColorJitter(brightness=0.8,\n",
        "                    contrast=0.8,saturation=0.8,hue=0.2)],p=0.8),\n",
        "\n",
        "            #converts the PIL to a tensor\n",
        "            transforms.ToTensor(),\n",
        "            #transforms.RandomGrayscale(p=0.2)\n",
        "            # #grayscele (1 channel) then duplicate the channel 3 times\n",
        "            # transforms.Lambda(lambda x: torch.cat([x, x, x], 0) \\\n",
        "            #                   if x.size(0) == 1 else x),\n",
        "            #applies Gaussian blur with a 3*3 kernel to image with 50% chance\n",
        "            transforms.RandomApply([transforms.GaussianBlur(3)], p=0.5),\n",
        "            #normalizes the image tensor with certain mean and std\n",
        "            transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "        ])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.imgs.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # TASK: Fill in the blanks such that you return two tensors\n",
        "        # of shape [1, 28, 28], img_view1 and img_view2, representing two\n",
        "        #augmented view of the images.\n",
        "        ...\n",
        "        img = self.imgs[index]\n",
        "        img_view1 = self.augmentation_pipeline(img)\n",
        "        img_view2 = self.augmentation_pipeline(img)\n",
        "\n",
        "        return img_view1, img_view2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwqYUOutSOGK"
      },
      "source": [
        "We use a [LightningDataModule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html) for handling your PneumoniaMNIST dataset. You do not need to make any modifications to the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQTVOXwESOGL"
      },
      "outputs": [],
      "source": [
        "class SimCLRPneumoniaMNISTDataModule(LightningDataModule):\n",
        "    def __init__(self, batch_size: int = 8):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.train_set = SimCLRPneumoniaMNISTDataset(split='train')\n",
        "        self.val_set = SimCLRPneumoniaMNISTDataset(split='val')\n",
        "        self.test_set = SimCLRPneumoniaMNISTDataset(split='test')\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(dataset=self.train_set, batch_size=self.batch_size,\n",
        "                          shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(dataset=self.val_set, batch_size=self.batch_size,\n",
        "                          shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(dataset=self.test_set, batch_size=self.batch_size,\n",
        "                          shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxDF8J8CSOGL"
      },
      "source": [
        "#### **Check** dataset implementation.\n",
        "\n",
        "Run the below cell to visualise a batch of your training dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbV-xhDitmrR"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL! IT IS FOR CHECKING THE IMPLEMENTATION ONLY.\n",
        "\n",
        "# Initialise data module\n",
        "datamodule = SimCLRPneumoniaMNISTDataModule()\n",
        "# Get train dataloader\n",
        "train_dataloader = datamodule.train_dataloader()\n",
        "# Get first batch\n",
        "batch = next(iter(train_dataloader))\n",
        "# Visualise the images\n",
        "view1, view2 = batch\n",
        "f, ax = plt.subplots(2, 8, figsize=(12,4))\n",
        "for i in range(8):\n",
        "  ax[0,i].imshow(view1[i, 0], cmap='gray')\n",
        "  ax[1,i].imshow(view2[i, 0], cmap='gray')\n",
        "  ax[0,i].set_title('view 1')\n",
        "  ax[1,i].set_title('view 2')\n",
        "  ax[0, i].axis(\"off\")\n",
        "  ax[1, i].axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9dmJ2CV8i3o"
      },
      "source": [
        "## **Part B:** Implement the SimCLR loss and training step.\n",
        "\n",
        "In this part, we ask you to:\n",
        "1. Implement the SimCLR loss function, as per the equation in the lecture notes (and the [original paper](https://arxiv.org/abs/2002.05709)).\n",
        "2. Once you have implemented the loss, implement the training step function in the provided LightningModule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg3W4Mh8anKn"
      },
      "source": [
        "### **Task B-1:** SimCLR loss function.\n",
        "\n",
        "For the implementation of the SimCLR loss, you should follow the 'recipe' from the lecture slides. We provide a code skeleton to get you started. Fill in all the blanks.\n",
        "\n",
        "*Hint:* In PyTorch, to compute scalar products (also called dot products) between many elements efficiently, note that for two batches of $d$-dimensional feature vectors $v1$ and $v2$ of size $[N, d]$ (with $N$ being the batch size) computing the matrix multiplication `torch.mm(v1, v2.t())` returns a matrix $S$ of size $[N, N]$ where each element $S[i, j]$ is the scalar product of $v1_i$ and $v2_j$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjS5kMqsFPZg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def simclr_loss(embedding_view1, embedding_view2, tau=1.0):\n",
        "    # Step 1: Normalize the embeddings\n",
        "    embedding_view1 = F.normalize(embedding_view1, dim=1)\n",
        "    embedding_view2 = F.normalize(embedding_view2, dim=1)\n",
        "\n",
        "    # Step 2: gather all embeddings into one big vector of size [2*N,feature_dim]\n",
        "    z_all_views = torch.cat([embedding_view1, embedding_view2], dim=0)\n",
        "\n",
        "    # Step 3: compute all possible similarities,\n",
        "             #should be a matrix of size [2 * N, 2 * N]\n",
        "    # all_similarities[i,j] will be the similarity between z_all_views[i] and\n",
        "    #z_all_views[j].\n",
        "    # Use the hint.\n",
        "    all_similarities = torch.mm(z_all_views, z_all_views.t())\n",
        "\n",
        "    # Step 4: Here we want to return a mask of size[2 * N, 2* N] for which\n",
        "    #mask[i,j] = 1 if z_all_views[i] and z_all_views[j] form a positive pair.\n",
        "    # There should be exactely 2 * N non-zeros elements in this matrix.\n",
        "    batch_size = embedding_view1.size(0)\n",
        "    masks_pre = torch.cat([torch.arange(batch_size) for _ in range(2)], dim=0)\n",
        "    #element-wise comparison\n",
        "    boolean_pair = masks_pre.unsqueeze(0) == masks_pre.unsqueeze(1)\n",
        "    positive_pair_mask = boolean_pair.float().to(embedding_view1.device)\n",
        "    positive_pair_mask.fill_diagonal_(0) # Exclude self-similarities\n",
        "\n",
        "    # Step 5: self-mask. For computing the denominator term in the loss function,\n",
        "    # we need to sum over all possible similarities except the self-similarity.\n",
        "    # Create a mask of shape [2*N, 2*N] that is 1 for all valid pairs and 0 for\n",
        "    #all self-pairs (i = j).\n",
        "    mask_exclude_self = 1 - torch.eye(\n",
        "        2 * batch_size, device=embedding_view1.device)\n",
        "\n",
        "    # Step 6: Computing all numerators for the loss function.\n",
        "    # Should be vector of size [2 * N],\n",
        "    # where element is exp(sim(i, j) / t) for each positive pair (i, j).\n",
        "    # Re-use the computed quantities above.\n",
        "    numerators_all = torch.exp(all_similarities / tau) *\n",
        "    positive_pair_mask.to(embedding_view1.device)\n",
        "\n",
        "    # Step 7: Computing all denominators for the loss function.\n",
        "    # Should be a vector of size [2 * N].\n",
        "    # Where each element should be the sum of exp(sim(i,k)/tau) for all k != i.\n",
        "    denominators_all = torch.sum(torch.exp(all_similarities / tau) *\n",
        "              mask_exclude_self, dim=1, keepdim=True).to(embedding_view1.device)\n",
        "\n",
        "\n",
        "    # Step 8: Return the final loss values, using the previously\n",
        "    #computing numerators and denominators.\n",
        "    loss = -torch.log(\n",
        "        numerators_all[positive_pair_mask.bool()] / (denominators_all + 1e-11))\n",
        "    loss = loss.mean()\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHkOZiucO3-D"
      },
      "source": [
        "#### **Check** SimCLR loss function.\n",
        "\n",
        "To check your implementation, please run the following tests. Note that we will also use other tests on different inputs to test your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orohu5xQO3KA"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL! IT IS FOR CHECKING THE IMPLEMENTATION ONLY.\n",
        "\n",
        "seed_everything(33)\n",
        "\n",
        "expected_results = [torch.tensor(1.7518), torch.tensor(1.6376),\n",
        "                    torch.tensor(4.194),  torch.tensor(4.1754)]\n",
        "for i, (N, feature_dim) in enumerate(zip([3, 3, 33, 33], [5, 125, 5, 125])):\n",
        "  embedding_view1 = torch.rand((N, feature_dim))\n",
        "  embedding_view2 = torch.rand((N, feature_dim))\n",
        "  loss = simclr_loss(embedding_view1.clone(), embedding_view2.clone(), tau=0.5)\n",
        "  print(f\"Expected loss: {expected_results[i]}, Computed loss: {loss}\")\n",
        "  assert torch.isclose(loss, expected_results[i], rtol=1e-3)\n",
        "print(\"Passed all tests successfully !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir0Myj1n-q_G"
      },
      "source": [
        "### **Task B-2:** SimCLR training step.\n",
        "\n",
        "In this next task you are asked to complete the blanks in the provided [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html).\n",
        "\n",
        "We provide the implementation of an image encoder (the CNN backbone that will act as feature extractor). No changes are needed for this part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u77dpCujSOGO"
      },
      "outputs": [],
      "source": [
        "class ImageEncoder(torch.nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.net = models.resnet50(weights=None)\n",
        "        del self.net.fc\n",
        "        self.net.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2,\n",
        "                                         padding=3, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.net.conv1(x)\n",
        "        x = self.net.bn1(x)\n",
        "        x = self.net.relu(x)\n",
        "        x0 = self.net.maxpool(x)\n",
        "        x1 = self.net.layer1(x0)\n",
        "        x2 = self.net.layer2(x1)\n",
        "        x3 = self.net.layer3(x2)\n",
        "        x4 = self.net.layer4(x3)\n",
        "        x4 = self.net.avgpool(x4)\n",
        "        x4 = torch.flatten(x4, 1)\n",
        "        return x4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ih6SONfSOGO"
      },
      "source": [
        "Next, you will need to complete the implementation of the SimCLR model. In order to make the training step work correctly, you will need to implement the `process_batch` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sizIiNm8i3o"
      },
      "outputs": [],
      "source": [
        "class SimCLRModel(LightningModule):\n",
        "    def __init__(self, learning_rate: float = 0.001):\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.encoder = ImageEncoder()\n",
        "\n",
        "        self.projector = torch.nn.Sequential(\n",
        "            torch.nn.Linear(2048, 1024),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(1024, 128),\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    def process_batch(self, batch):\n",
        "        # TASK: Implement the process_batch function\n",
        "\n",
        "        view1,view2 = batch\n",
        "        h_1 = self.encoder(view1)\n",
        "        h_2 = self.encoder(view2)\n",
        "        z_1 = self.projector(h_1)\n",
        "        z_2 = self.projector(h_2)\n",
        "        loss = simclr_loss(z_1, z_2)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self.process_batch(batch)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        if batch_idx == 0:\n",
        "            grid = torchvision.utils.make_grid(torch.cat((batch[0][0:4, ...],\n",
        "                            batch[1][0:4, ...]), dim=0), nrow=4, normalize=True)\n",
        "            self.logger.experiment.add_image('train_images', grid,\n",
        "                                             self.global_step)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self.process_batch(batch)\n",
        "        self.log('val_loss', loss, prog_bar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgSl_FnT_H0B"
      },
      "source": [
        "#### **Check** SimCLR training step.\n",
        "\n",
        "Here you can test that your code runs fine by training the model for 5 epochs using the cell below.\n",
        "\n",
        "Report the training and validation loss at the end of 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXksBOrR8i3p"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL! IT IS FOR CHECKING THE IMPLEMENTATION ONLY.\n",
        "\n",
        "seed_everything(33, workers=True)\n",
        "\n",
        "data = SimCLRPneumoniaMNISTDataModule(batch_size=32)\n",
        "\n",
        "model = SimCLRModel()\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=5,\n",
        "    accelerator='auto',\n",
        "    devices=1,\n",
        "    logger=TensorBoardLogger(save_dir='./lightning_logs/coursework/',\n",
        "                             name='simclr'),\n",
        "    callbacks=[ModelCheckpoint(monitor='val_loss', mode='min'),\n",
        "               TQDMProgressBar(refresh_rate=10)],\n",
        ")\n",
        "trainer.fit(model=model, datamodule=data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRUmT50C8i3p"
      },
      "source": [
        "## **Part C:** Linear probing and model finetuning.\n",
        "\n",
        "In this part, you are given two different image encoders that were pre-trained with different datasets and training strategies. The objective for this task is to assess the performance of these two encoders in a downstream classification task. This this end, you are asked to implement evaluation routines seen in the lecture: linear probing and model finetuning. The downstream task is the prediction of Pneumonia in the (small) chest X-ray images from the PneumoniaMNIST dataset.\n",
        "\n",
        "This part can be broken down into the following tasks:\n",
        "1. Adapt your PneunomiaMNIST dataset for the image classification task.\n",
        "2. Implement a classification model with a linear layer attached to a pre-trained image encoder.\n",
        "3. For both pre-trained encoders:\n",
        "    - a) Train the classifier on top of the frozen encoder (linear probing)\n",
        "    - b) Finetune the entire model (including the encoder).\n",
        "4. Evaluate all models on the test set, and provide a brief summary (no more than 300 words) with an analysis of your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8eiyVBc_429"
      },
      "source": [
        "### **Task C-1:** Adapt your PneunomiaMNIST dataset for the image classification task.\n",
        "\n",
        "We can base our implementation largely on the `SimCLRPneumoniaMNISTDataset` and adapt it to make it suitable for image classification. Think about a suitable data augmentation pipeline. Check previous tutorials for inspiration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5WXLUOtBpw_"
      },
      "outputs": [],
      "source": [
        "class PneumoniaMNISTDataset(MedMNIST):\n",
        "    def __init__(self, split = 'train', augmentation: bool = False):\n",
        "        ''' Dataset class for Pneumonia MNST.\n",
        "        The provided init function will automatically download the necessary\n",
        "        files at the first class initialistion.\n",
        "\n",
        "        :param split: 'train', 'val' or 'test', select subset\n",
        "\n",
        "        '''\n",
        "        self.flag = \"pneumoniamnist\"\n",
        "        self.size = 28\n",
        "        self.size_flag = \"\"\n",
        "        self.root = './data/coursework/'\n",
        "        self.info = INFO[self.flag]\n",
        "        self.download()\n",
        "\n",
        "        npz_file = np.load(os.path.join(self.root, \"pneumoniamnist.npz\"))\n",
        "\n",
        "        self.split = split\n",
        "\n",
        "        # Load all the images\n",
        "        assert self.split in ['train','val','test']\n",
        "\n",
        "        self.imgs = npz_file[f'{self.split}_images']\n",
        "        self.labels = npz_file[f'{self.split}_labels']\n",
        "\n",
        "        self.do_augment = augmentation\n",
        "\n",
        "        # TASK: Define here your data augmentation pipeline suitable for\n",
        "        #classification.\n",
        "        # Check previous tutorials for inspiration.\n",
        "        self.transform = transforms.Compose([\n",
        "                  transforms.ToPILImage(),\n",
        "                  # crop random part of the image and then resize back\n",
        "                  transforms.RandomResizedCrop(self.size, scale=(0.77, 0.92)),\n",
        "                  # flip image horizontally with 50% probability\n",
        "                  transforms.RandomHorizontalFlip(),\n",
        "                  transforms.ToTensor(),\n",
        "                  ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.imgs.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # TASK: Implement the __getitem__ function to return the image and its\n",
        "        #class label.\n",
        "        img = self.imgs[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        if self.do_augment:\n",
        "          img = self.transform(img)\n",
        "        else:\n",
        "          transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                          transforms.ToTensor()])\n",
        "          img = transform(img)\n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVd7l4PiSOGQ"
      },
      "source": [
        "Again, we use a [LightningDataModule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html) for handling your PneumoniaMNIST dataset. No changes needed for this part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_5yidccSOGQ"
      },
      "outputs": [],
      "source": [
        "class PneumoniaMNISTDataModule(LightningDataModule):\n",
        "    def __init__(self, batch_size: int = 32):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.train_set = PneumoniaMNISTDataset(split='train', augmentation=True)\n",
        "        self.val_set = PneumoniaMNISTDataset(split='val', augmentation=False)\n",
        "        self.test_set = PneumoniaMNISTDataset(split='test', augmentation=False)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(dataset=self.train_set, batch_size=self.batch_size,\n",
        "                          shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(dataset=self.val_set, batch_size=self.batch_size,\n",
        "                          shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(dataset=self.test_set, batch_size=self.batch_size,\n",
        "                          shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTiwSxwSF3Pd"
      },
      "source": [
        "#### **Check** dataset implementation.\n",
        "\n",
        "Run the below cell to visualise a batch of your training dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IESom_gkF3Pd"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL! IT IS FOR CHECKING THE IMPLEMENTATION ONLY.\n",
        "\n",
        "# Initialise data module\n",
        "datamodule = PneumoniaMNISTDataModule()\n",
        "# Get train dataloader\n",
        "train_dataloader = datamodule.train_dataloader()\n",
        "# Get first batch\n",
        "batch = next(iter(train_dataloader))\n",
        "# Visualise the images\n",
        "images, labels = batch\n",
        "f, ax = plt.subplots(1, 8, figsize=(12,4))\n",
        "for i in range(8):\n",
        "  ax[i].imshow(images[i, 0], cmap='gray')\n",
        "  ax[i].set_title('label: ' + str(labels[i].item()))\n",
        "  ax[i].axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU2tN86DChgL"
      },
      "source": [
        "### **Task C-2:** Implement a classification model with a linear layer attached to a pre-trained image encoder.\n",
        "\n",
        "We first download the weights of the two pre-trained image encoders. One of them has been trained with the self-supervised SimCLR objective on a large publicly available chest X-ray dataset (different from PneunomiaMNIST). The other encoder is a standard ImageNet backbone that has been trained with a supervised classification objective on the ImageNet dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gX9NTQqSOGR"
      },
      "outputs": [],
      "source": [
        "! wget https://www.doc.ic.ac.uk/~bglocker/teaching/mli/coursework.zip\n",
        "! unzip coursework.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KCcuk9ISOGR"
      },
      "source": [
        "We provide the function for loading the encoders. No changes needed here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iHkCTqLETVr"
      },
      "outputs": [],
      "source": [
        "def load_encoder_from_checkpoint(checkpoint_path):\n",
        "  ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
        "  simclr_module = SimCLRModel()\n",
        "  print(simclr_module.load_state_dict(state_dict=ckpt))\n",
        "  return simclr_module.encoder.eval()\n",
        "\n",
        "imagenet_model = './data/coursework/model_imagenet.ckpt'\n",
        "chestxray_model = './data/coursework/model_chestxray.ckpt'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otccmykgSOGS"
      },
      "source": [
        "Now, implement a classification model as a LightningModule for image classification using a pre-trained image encoder.\n",
        "\n",
        "The model should have a flag in the init function `freeze_encoder` that if set to true freezes all the weights in the encoder (used for linear probing), and if set to false all weights are trainable (used for model finetuning).\n",
        "\n",
        "*Hint:* Check out previous tutorials for inspiration on how to implement a classification model as LightningModule. For the coursework, we recommend using the Area Under the Receiver Operating Characteristic Curve (ROC-AUC) performance metric (instead of accuracy). ROC-AUC is measure of the overall discriminative power of a classification model. You can use the readily available implementation in [torchmetrics](https://lightning.ai/docs/torchmetrics/stable/classification/auroc.html#functional-interface). You should log the ROC-AUC similar to how we logged accuracy in previous tutorials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_lHEp69DVqt"
      },
      "outputs": [],
      "source": [
        "# TASK: Implement the ImageClassifier class\n",
        "# Check previous tutorials for insipration how to implement an `ImageClassifier`\n",
        "\n",
        "class ImageClassifier(LightningModule):\n",
        "    def __init__(self, pretrained_encoder: torch.nn.Module,\n",
        "                 freeze_encoder: bool = True, output_dim: int = 2,\n",
        "                 learning_rate: float = 0.001):\n",
        "        super().__init__()\n",
        "        self.encoder = pretrained_encoder\n",
        "        self.flag = freeze_encoder\n",
        "        self.output_dim = output_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        if self.flag:\n",
        "          for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.ln = nn.Linear(2048, self.output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      #pass linearised x through the encoder\n",
        "      encoding = self.encoder(x)\n",
        "      #pass embedding through the fully connected layers\n",
        "      pre = self.ln(encoding)\n",
        "\n",
        "      return pre\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    def process_batch(self, batch):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        y = y.reshape(y.size(0))\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        roc_auc = auroc(logits, y, task='multiclass', num_classes=self.output_dim)\n",
        "        return loss, roc_auc\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, roc_auc = self.process_batch(batch)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        self.log('train_roc_auc', roc_auc, prog_bar=True)\n",
        "        if batch_idx == 0:\n",
        "            grid = torchvision.utils.make_grid(batch[0][0:16, ...],\n",
        "                                               nrow=4, normalize=True)\n",
        "            self.logger.experiment.add_image('train_images', grid, self.global_step)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, roc_auc = self.process_batch(batch)\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('val_roc_auc', roc_auc, prog_bar=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss, roc_auc = self.process_batch(batch)\n",
        "        self.log('test_loss', loss)\n",
        "        self.log('test_roc_auc', roc_auc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTN7khmEFVba"
      },
      "source": [
        "### **Task C-3a:** Implement training and testing for linear probing.\n",
        "\n",
        "Train two classification models using linear probing, one for each of the two provided image encoders. Evaluate on both the validation and test sets.\n",
        "\n",
        "*Note:* Training for 25 epochs should be sufficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvjW7rGZFU1m"
      },
      "outputs": [],
      "source": [
        "seed_everything(33, workers=True)\n",
        "\n",
        "data = PneumoniaMNISTDataModule(batch_size=32)\n",
        "\n",
        "# TASK: Implement the linear probing training and testing routines.\n",
        "\n",
        "# use imagenet_model\n",
        "print(\"------------------Implement imagenet_model------------------\")\n",
        "imagenet_encode = load_encoder_from_checkpoint(imagenet_model)\n",
        "model = ImageClassifier(imagenet_encode, freeze_encoder=True, output_dim=2,\n",
        "                        learning_rate=0.001)\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=25,\n",
        "    accelerator='auto',\n",
        "    devices=1,\n",
        "    logger=TensorBoardLogger(save_dir='./lightning_logs/linear/classification/',\n",
        "                             name='imagenet_model'),\n",
        "    callbacks=[ModelCheckpoint(monitor='val_loss', mode='min'),\n",
        "               TQDMProgressBar(refresh_rate=10)],\n",
        ")\n",
        "trainer.fit(model=model, datamodule=data)\n",
        "\n",
        "#use chestxray_model\n",
        "print(\"------------------Implement chestxray_model------------------\")\n",
        "chestxray_encode = load_encoder_from_checkpoint(chestxray_model)\n",
        "model = ImageClassifier(chestxray_encode, freeze_encoder=True, output_dim=2,\n",
        "                        learning_rate=0.001)\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=25,\n",
        "    accelerator='auto',\n",
        "    devices=1,\n",
        "    logger=TensorBoardLogger(save_dir='./lightning_logs/linear/classification/',\n",
        "                             name='chestxray_model'),\n",
        "    callbacks=[ModelCheckpoint(monitor='val_loss', mode='min'),\n",
        "               TQDMProgressBar(refresh_rate=10)],\n",
        ")\n",
        "trainer.fit(model=model, datamodule=data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDa5-HnyHoRh"
      },
      "source": [
        "### **Task C-3b:** Implement training and testing for model finetuning.\n",
        "\n",
        "Repeat the experiments, but this time using model finetuning instead of linear probing. Evaluate on both the validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF2RYgemHdf-"
      },
      "outputs": [],
      "source": [
        "seed_everything(33, workers=True)\n",
        "\n",
        "data = PneumoniaMNISTDataModule(batch_size=32)\n",
        "\n",
        "# TASK: Implement the model finetuning training and testing routines.\n",
        "# use imagenet_model\n",
        "print(\"------------------Implement imagenet_model------------------\")\n",
        "imagenet_encode = load_encoder_from_checkpoint(imagenet_model)\n",
        "model = ImageClassifier(imagenet_encode, freeze_encoder=False,\n",
        "                        output_dim=2, learning_rate=0.001)\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=25,\n",
        "    accelerator='auto',\n",
        "    devices=1,\n",
        "    logger=TensorBoardLogger(save_dir='./lightning_logs/finetuning/classification/',\n",
        "                             name='imagenet_model'),\n",
        "    callbacks=[ModelCheckpoint(monitor='val_loss', mode='min'),\n",
        "               TQDMProgressBar(refresh_rate=10)],\n",
        ")\n",
        "trainer.fit(model=model, datamodule=data)\n",
        "\n",
        "#use chestxray_model\n",
        "print(\"------------------Implement chestxray_model------------------\")\n",
        "chestxray_encode = load_encoder_from_checkpoint(chestxray_model)\n",
        "model = ImageClassifier(chestxray_encode, freeze_encoder=False,\n",
        "                        output_dim=2, learning_rate=0.001)\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=25,\n",
        "    accelerator='auto',\n",
        "    devices=1,\n",
        "    logger=TensorBoardLogger(save_dir='./lightning_logs/finetuning/classification/',\n",
        "                             name='chestxray_model'),\n",
        "    callbacks=[ModelCheckpoint(monitor='val_loss', mode='min'),\n",
        "               TQDMProgressBar(refresh_rate=10)],\n",
        ")\n",
        "trainer.fit(model=model, datamodule=data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9vluYnwSOGT"
      },
      "source": [
        "### **Task C-4:** Your evaluation report.\n",
        "\n",
        "Provide a brief summary (no more than 300 words) with an analysis of your findings. Try explaining the observed performance.\n",
        "\n",
        "### Validation Performance Analysis:\n",
        "1. **ROC AUC Scores**:\n",
        "   - The ROC AUC validation scores for ImageNet and Chest X-ray models (above 0.9 in both linear probing and fine-tuning) suggest that they are effective in distinguishing between classes. The ImageNet improves its ROC AUC from 0.971 in linear probing to 0.992 in fine-tuning, while the Chest X-ray has increased scores from 0.932 to 0.991, respectively. These improvements indicate that fine-tuning can faciliate model to capture subtle image pattern, thereby improving model's accuracy on validation dataset.\n",
        "  \n",
        "2. **Validation Loss**:\n",
        "    - The validation loss provides a direct measure of the model's error on unseen data. In the linear probing, the ImageNet and Chest X-ray reported validation losses of 0.194 and 0.315, respectively. In the fine-tuning, where all parameters were trainable, the validation losses increased for the ImageNet (to 0.361) but decreased for the Chest X-ray (to 0.158). The increase in validation loss for the ImageNet could indicate that the model, despite being more flexible, might be starting to overfit the training data, reducing its generalization capability. Conversely, the decrease in validation loss for the Chest X-ray suggests that additional training of the full model could effectively capture the relevant features from the pnuemonia images without compromising its ability to generalize.\n",
        "\n",
        "### Zero ROC AUC Analysis:\n",
        "  - Regarding the unexpected zero ROC AUC observed in some training batches for the ImageNet encoder during both linear probing and fine-tuning, it's probable that this encoder struggles to learn certain distributions present in those batches. Consequently, the encoder may classify all data points in a batch into a single class (0 or 1), resulting in a zero ROC AUC and a significant increase in loss. However, the ImageNet encoder performs well on the validation dataset, possibly because it is well-suited to that particular dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62TRONJYF3Pq"
      },
      "source": [
        "## Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUjzDI-iF3Pq"
      },
      "outputs": [],
      "source": [
        "#%load_ext tensorboard\n",
        "#%tensorboard --logdir './lightning_logs/coursework/'\n",
        "#%tensorboard --logdir './lightning_logs/classification'\n",
        "#%tensorboard --logdir './lightning_logs/finetuning'\n",
        "#%tensorboard --logdir './lightning_logs/linear'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}